# Threads & Synchronization â€“ Squareâ€‘Root Summation Benchmark

This project investigates **three different synchronization strategies** for multiâ€‘threaded computations in C using POSIXÂ threads. The task: compute the sum

\[ S = \sum_{i=a}^{b} \sqrt{i} \]

for a large integer range `[a,b]`, splitting the work across *N* threads.

> Course: **CSEÂ 3033 Operating Systems** â€“ ProjectÂ 3  
> Report: `CSEÂ 3033 PROJECTÂ 3 REPORT.docx`

---

## ğŸ“‚ Repository Layout

| File | Description |
|------|-------------|
| `Project3.c` | Singleâ€‘file implementation containing the three methods & timing harness |
| `CSE 3033 PROJECT 3 REPORT.docx` | Detailed analysis, tables & screenshots |

---

## ğŸ§µ Methods Implemented

| Method | Approach | Synchronization | Expected Behaviour |
|--------|----------|-----------------|--------------------|
| **method1** | Each thread independently accumulates *into the single global variable* | **None** (data race) | Fastest raw speed but **incorrect** sum due to race conditions |
| **method2** | Threads add directly to *global* but guard each addition | `pthread_mutex_lock` each loop iteration | Correct but high contention â†’ slow |
| **method3** | Each thread keeps a *local* partial sum, mutexâ€‘adds once at the end | mutex only once per thread | Correct **and** fastest among correct methods |

Threads are created by `executeMethod{1,2,3}()`; range splitting is equal (`rangePerThread = (bâ€‘a+1)/N`).

---

## â± Building & Running
### Compile
```bash
gcc -O2 -Wall -pthread -o sqrt_sum Project3.c
```
### Run
```bash
./sqrt_sum <a> <b> <num_threads>
```
*Example:*
```bash
./sqrt_sum 1 100000000 8
```
Outputs timings like:
```
Method 1 (no sync)      : 0.12 s  (wrong result!)
Method 2 (mutex per add): 5.48 s
Method 3 (local + mutex): 0.89 s
Correct value           : 21081851002.349...
```
*(Exact numbers depend on CPU.)*

---

## ğŸ“ˆ Key Findings (from report)
* **Race conditions** in MethodÂ 1 produce up to 40Â % error on 16â€‘thread runs.
* Mutex per addition in MethodÂ 2 serializes access, eliminating parallel benefit.
* Aggregating locally (MethodÂ 3) scales nearly linearly until ~core count.

| Threads | M1 time | M2 time | M3 time |
|---------|---------|---------|---------|
| 1 | 0.95Â s | 0.96Â s | 0.95Â s |
| 4 | 0.27Â s | 3.65Â s | 0.25Â s |
| 8 | 0.15Â s | 5.48Â s | 0.13Â s |
| 16 | 0.13Â s | 9.90Â s | 0.12Â s |

*(Hardware: RyzenÂ 7Â 5800H)*

---

## ğŸ”¬ Extension Ideas
1. Replace mutex with **atomic fetchâ€‘add** (`__sync_fetch_and_add`) for MethodÂ 2.
2. Use **OpenMP reduction** clause and compare overhead.
3. Experiment with **false sharing** by padding local sums to cacheâ€‘line size.
4. Port to **Windows** using `<windows.h>` `CRITICAL_SECTION`.

---

## ğŸ¯ Takeaways
* Always minimise critical section length; aggregate locally when possible.
* Measure! Synchronization overhead can dwarf computation for tiny operations.

